Install aws cli
    #cd ~/Downloads/
    #curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip
    #unzip awscliv2.zip 
    #sudo ./aws/install
    #aws version
    #aws help

For the awscli to access our aws account, we need configure it with some credentials.
AccessKey and SecretAccessKey are required as credentials. Username and Password cannot be used.
--NEVER-- create AccessKey for your aws root account.
Create a new IAM user that has fullaccess only in S3 and IAM. On your browser, access the portal at https://us-east-1.console.aws.amazon.com/iam/home#/users/create
    ->Step1 : Provide the name "iam_admin". --uncheck-- "access to the AWS Management Console"
    ->Step2 : Attach policies directly. Search for S3 and select the policy that grants fullaccess. Do the same for IAM
    ->Step3 : Review then "Create User"
From the users list, Select the user that you have created.
Go to the "Security Credentials" tab. Click on "Create access key". Go through the steps.
Download the .csv of your accesskey and secret to a safe location on your system. OR, stay on the page until awscli config is done.
--DO NOT-- use a shared directory, network directory or a git repo directory as the download destination.
Lets configure awscli with this access
    #aws configure help
    #aws configure
        ->Paste the accesskey and secret according to the prompts
        ->Enter the aws region from which is expected to have most operations
        ->Specify the format awscli shoud use while responding to queries, ie, json, text, or table
    #aws configure list
    #cat ~/.aws/config
    #cat ~/.aws/credentials
The accesskey, secret, region, output are now associated with the "default" profile.
Any number of accesskeys and secrets pairs can be configure under unique profilenames.
The only way to remove or delete any set of credentials is by editing this credentials file.
But you can overwrite the credentials of a profile by running the import command again.
If the accesskey and secrets was downloaded as a csv file, it can be used in the import command.
    #aws configure import --csv file://iam_admin_accessKeys.csv
This command may fail, but can be fixed by editing the csv file
    #vim aws_userforiam_accessKeys.csv
        ->Prepend first line with "User Name"
        ->Prepend second line with "default" or with the username
    #aws configure import --csv file://iam_admin_accessKeys.csv
    #aws configure list-profiles
    #aws configure list
        ->If you have not used "default", use the flag --profile=username
    #cat ~/.aws/credentials
To interactively set accesskeys and secrets for the profile foo
    #aws configure --profile foo

Lets verify if the credentials work. Lets list create and destroy an s3 bucket
    #aws s3 help
    #aws s3 ls
    #aws s3 mb "s3://mynewbucket-willwork"
        ->If default region was not configured, use the flag --region=us-east-1
You can check aws console if the bucket was created or not
    #aws s3 ls
    #aws s3 rb "s3://mynewbucket-willwork"
    #aws s3 ls

Lets start using terraform to create and manage resources in this aws account of ours.
    #mkdir aws && cd aws
Use the aws provider and set it up with shared config and creds. Refer https://registry.terraform.io/providers/hashicorp/aws/latest/docs#shared-configuration-and-credentials-files
    #vim providers.tf
The config in aws providers block is optional if you are using default profile and default paths.
This would also be a good spot to add default_tags that you would want all your resources to inherit.
Initialize the project by installing the provider plugin
    #terraform init
Browse the docs for resources and data sources supported by the aws provider.

Lets start off by managing an S3 bucket. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket
Create your first S3 bucket using the aws_s3_bucket resource with only its bucket argument.
    #vim s3.tf
    #terraform apply
If the bucket name is not globally unique, you will see errors like "BucketAlreadyExists" or "Header malformed : Wrong region"
Disable/Comment-out the first aws_s3_bucket resource.
Create the second aws_s3_bucket resource with bucket_prefix argument.
The bucket_prefix argument, guarentees uniqueness by appending a 26 digit number to it.
The terraform aws provider generates a time based unique random number that has the format YYYYMMDDhhmmssXXXX00000001.
Include a hyphen at the end of the prefix so that the bucket_name looks nice.
Use the force_destroy argument so that we can still destroy or recreate even if bucket is not empty or is processing uploads/downloads.
    #vim s3.tf
    #terraform apply
    #terraform state list
    #terraform state show aws_s3_bucket.second
    #aws s3api list-buckets
Create output block named mys3_id_second to display the name of the s3 bucket that was created
    #terraform console
        #aws_s3_bucket.second.id
    #vim output.tf
    #terraform apply
    #terraform output
    #terraform output mys3_id_second
The tags argument in aws_s3_bucket resource allows us to add new tags and/or override the values of any default_tags.
    #vim s3.tf
    #terraform apply
    #terraform state show aws_s3_bucket.second
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_s3_bucket.second") | .values | with_entries( select( .key | startswith("tag") ) )'
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_s3_bucket.second") | .values.tags'
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_s3_bucket.second") | .values.tags_all'
    #aws s3api get-bucket-tagging help
    #aws s3api get-bucket-tagging --bucket $(aws s3api list-buckets --query "Buckets[].Name" --output text)
    #aws s3api get-bucket-tagging --bucket $(terraform output -raw mys3_id_second )
    #terraform output -raw mys3_id_second | xargs -I?? aws s3api get-bucket-tagging --bucket ??
    #terraform output -raw mys3_id_second | { read OUT; aws s3api get-bucket-tagging --bucket $OUT; }
Create variable blocks mys3_prefix, bucket_force_destroy and bucket_extra_tags to handle the inputs.
Set default values for each variable. Its safer to set force_destroy as false but lets use true for our training project.
    #vim variable.tf
    #terraform console
        #var.mys3_prefix
        #var.bucket_force_destroy
        #var.bucket_extra_tags
Create a local mys3_prefix_hyphen thats just a hyphenated version of mys3_prefix variable
Update the argument values of aws_s3_bucket to use these variables and locals.
    #vim s3.tf
    #terraform console
        #local.mys3_prefix_hyphen
    #terraform apply
We will look at bucket versioning and uploading into the bucket later.

S3 buckets need globally unique name. We got this by adding a time+random number to the name using bucket prefix.
Lets use Terraforms Random provider to create random resources that can be applied wherever we need uniqueness. https://registry.terraform.io/providers/hashicorp/random/latest/docs
    #vim providers.tf
    #terraform init
Lets create a random_id  resource with a byte_length of 7. byte_length is required and defines the size of the random generated binary code.
Lets create a random_pet resource with a length      of 3. length has a default value of 2 readable words.
    #vim s3.tf
    #terraform apply
    #terraform state show random_pet.third
    #terraform state show random_id.third
None of the attributes you see of random_id.third resource is the actual random code.
All attributes are encodings and/or conversions of some non-printable code.
I would be using the b64_url attribute as it has letters and numbers.
If you are curious to see the real code of random_id,
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="random_id.third") |.values'
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std'
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std' | base64 -d | hexdump -C
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std' | base64 -d | xxd -p
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std' | base64 -d | xxd -p | { read hex_val; echo $((16#$hex_val)); }
Creating an output block for random_id.third.b64_std would have allows us avoid the long jq. But we cannot isolate it when we use the prefix argument in random_id.
Set the prefix argument in random_id and random_pet resources with mys3_prefix_hyphen and mys3_prefix.
    #vim s3.tf
Lets also use variables to manage the length of random_id and random_pet
    #vim variables.tf
    #terraform console
        #var.suffix_string_length
        #var.suffix_word_count
    #terraform apply
Lets create new aws_s3_buckets whose names would be random_id.third.b64_url and random_pet.third.id. We can cycle through them using for each
Although the b64_url looks usable, s3 buckets names cannot have uppercase and/or underscore(_).
So, substitute "_" with "-" with the replace function and also use the lower function on it. https://developer.hashicorp.com/terraform/language/functions/replace
    #terraform console
        #random_id.third.b64_url
        #lower(replace(random_id.third.b64_url, "_", "-"))
        #toset(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"])
    #vim s3.tf
    #terraform apply
    #terraform state list
    #aws s3api list-buckets
Create an output block to return the names of the buckets created.
    #terraform console
        #aws_s3_bucket.third
        #values(aws_s3_bucket.third)[*].id
    #vim output.tf
    #terraform apply
    #terraform output mys3_id_third
    #aws s3api list-buckets
We will look at random_uuid later.

If you had followed along with you would have created 3 buckets without any terraform fail/error.
But there is fault that was hidden while terraform apply after each minor change. Lets uncover this fault
    #terraform destroy
    #terraform apply
Terraform apply will fail due to error at for_each of third aws_s3_buckets resource.
Arguments of a resource are allowed to refer attributes of other resources, this would be considered as an implicit dependency.
Terraform would delay the creation of the downstream dependent resource untill the upstream resource is created/modified and its attributes become available.
But terraform plan/apply fails here, when for_each the meta-argument is referring to resource attributes. Why?
(for_each + each.key) and (count + count.index) are evaluated and must be known before the creation of the dependency graph.
Their result decides what the names and labels of the resources must be, without which is dependency graph will be incomplete.
There are 3 solutions for this for_each error.
  Solution 1 : Avoid for_each
    We could create 2 seperate aws_s3_bucket blocks. One of them would get bucket name from random_id, and the Next one would use random_pet.
    I wont use this as it too simple for my taste.
  Solution 2 : Map of static keys:
    To correct the error we replace the set in for_each with a map of known/static keys.
        #terraform destroy
        #terraform console
            #toset(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"])
            #tomap({ third_id = "${lower(replace(random_id.third.b64_url, "_", "-"))}", third_pet = "${random_pet.third.id}" })
    Defining the map as a local will improve code readability. Name of the bucket should now be each.value
        #vim s3.tf
        #terraform apply
    Even though we switched from a set to a map in aws_s3_bucket.third, the value of output.mys3_id_third works with the same expression.
        #terraform state list
        #aws s3api list-buckets
        #terraform output
  Solution 3 : Count of known length
    Another solution is to replace the "for_each = toset()" with count whose value is the length of a list.
        #terraform destroy
        #terraform console
            #toset(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"])
            #length(tolist(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"]))
    Define the list as a local. Set the count as length(local.list) and the name of buckets as list items at count.index
        #vim s3.tf
        #terraform apply
    The value of output.mys3_id_third no longer needs the values() funtion.
        #terraform console
            #aws_s3_bucket.third[*].id
        #vim output.tf
        #terraform apply
        #terraform state list
        #aws s3api list-buckets
        #terraform output
Your choice on Solution 2 or 3 should depend on if your preferece of named vs numbered terraform resource maps. I prefer number so i choose solution 3.

We will be trying out random_uuid, bucket versioning and uploading into bucket soon.
But before that, I want to address a logical issue with the project so far.
To create a bucket with a unique name and we have used 3 methods.
And this resulted in the creation of three unique buckets, where we only needed one.
We need to be able choose one of three suffix methods to create one bucket.
Conditional Expression is our solution https://developer.hashicorp.com/terraform/language/expressions/conditionals.
Create a string type variable that enables us to choose the kind of suffix.
Apply a validation condition so that an error_message is shown for requests of unknown suffix_type. https://developer.hashicorp.com/terraform/language/validate
The contains function allows us to define a list of allowed suffix_type, ie, [all, none, time, words, string]. https://developer.hashicorp.com/terraform/language/functions/contains
    #vim variables.tf
    #terraform console
        #var.suffix_type
We use condition expression to check var.suffix_type. If it matches a keyword, return 1, else return 0.
    #terraform console
        #var.suffix_type=="none" ? true : false
        #var.suffix_type=="none" ? "on" : "off"
        #var.suffix_type=="none" ? 1 : 0
        #var.suffix_type=="string"||var.suffix_type=="all" ? 1 : 0
        #var.suffix_type=="words"||var.suffix_type=="all" ? 1 : 0
        #(var.suffix_type=="time"||var.suffix_type=="all") ? 1 : 0
As I plan to use count to process the conditional expression, I have used the numbers 1 and 0 as true_val and false_val.
You could use for_each to process the conditional expression, then use strings "on" and "off" as true_val and false_val.
We define count in aws_s3_bucket.first, aws_s3_bucket.second, random_id.third, random_pet.third to use their respective conditional expression.
Uncomment aws_s3_bucket.first and update its arguments to use var.mys3_prefix, var.bucket_force_destroy and var.bucket_extra_tags. Create an output block as well.
As a consequence of using count, the 4 individual resource objects have become 4 lists of objects with just one element each
So, update the values of output.mys3_id_first, output.mys3_id_second and local.id_list_third to use the 0th element of their resources.
    #vim s3.tf
We have not applied conditional expression in aws_s3_bucket.third because it is implicitly dependent on random_id and random_pet





Using aws_s3_bucket_versioning, we will enable versioning.
    #vim s3.tf
Or you could generate a UUID and append it to your bucket name. This is an AWS recommendion for naming resources.
    #uuidgen -s -n @oid -N "my-s3-bucket-name" | cut -d '-' -f 1
Create the resources and check if they have been created via console or cli
    #terraform apply
    #terraform state list
    #aws s3api list-buckets
    #aws s3api get-bucket-versioning --bucket <bucket_name>


Lets use S3 for maintaining the backend of our aws terraform projects. https://developer.hashicorp.com/terraform/language/backend/s3
If backend config is changed, you will have to perform init again.
To ignore previous
    #terraform init -reconfigure
To migrate-state
    #terraform init -force-copy

    #vim .terraformrc

Using the aws_s3_object resource lets upload files into this S3 bucket.
The bucket argument will be the destination of our file upload.
The key argument will be the path/filename within the bucket.
The source argument is the path of the file that we want to upload. Alternatively, you can use the content argument to write into key
    #vim s3.tf
Create the resources and check if they have been created via console or cli
    #terraform apply
    #terraform state list
    #aws s3api list-objects-v2 --bucket <bucket_name>

The .terraform folder contains the the plugin of the aws provider, which takes up 750Mb of disk space.

Using the aws_iam_user resource, create a user. name is a required argument. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_user
    #vim iam.tf
Create the resources and check if they have been created via console or cli
    #terraform apply
    #terraform state
    #aws iam list-users
    #aws iam get-user --user-name tf-user
Using aws_iam_access_key resource, create accesskey and secret. user is a required argument and should refer the name or id attribute of the aws_iam_user resource.
    #vim iam.tf
    #terraform apply
    #aws iam list-access-keys --user-name tf-user
    #terraform state show aws_iam_access_key.first
The aws_iam_access_key resource is created with its secret but is masked as it is a sensitive attribute. To see it,
    #less terraform.tfstate
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_iam_access_key.first") | .values.secret'

To generate aws user policies, Use https://awspolicygen.s3.amazonaws.com/policygen.html

The iam_admin user that we had create via aws console is only authorized to create IAM users and S3 buckets.
Terraform will use the iam_admin authorization to create IAM user with greater permissions.
The access of these codeified users will allow us to create and manage any resource in our aws account.
Lets create a project which we can use to manage new IAM users and their permission

