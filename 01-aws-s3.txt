Install aws cli
    #cd ~/Downloads/
    #curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip
    #unzip awscliv2.zip 
    #sudo ./aws/install
    #aws version
    #aws help

For the awscli to access our aws account, we need configure it with some credentials.
AccessKey and SecretAccessKey are required as credentials. Username and Password cannot be used.
--NEVER-- create AccessKey for your aws root account.
Create a new IAM user that has fullaccess only in S3 and IAM. On your browser, access the portal at https://us-east-1.console.aws.amazon.com/iam/home#/users/create
    ->Step1 : Provide the name "iam_admin". --uncheck-- "access to the AWS Management Console"
    ->Step2 : Attach policies directly. Search for S3 and select the policy that grants fullaccess. Do the same for IAM
    ->Step3 : Review then "Create User"
From the users list, Select the user that you have created.
Go to the "Security Credentials" tab. Click on "Create access key". Go through the steps.
Download the .csv of your accesskey and secret to a safe location on your system. OR, stay on the page until awscli config is done.
--DO NOT-- use a shared directory, network directory or a git repo directory as the download destination.
Lets configure awscli with this access
    #aws configure help
    #aws configure
        ->Paste the accesskey and secret according to the prompts
        ->Enter the aws region from which is expected to have most operations
        ->Specify the format awscli shoud use while responding to queries, ie, json, text, or table
    #aws configure list
    #cat ~/.aws/config
    #cat ~/.aws/credentials
The accesskey, secret, region, output are now associated with the "default" profile.
Any number of accesskeys and secrets pairs can be configure under unique profilenames.
The only way to remove or delete any set of credentials is by editing this credentials file.
But you can overwrite the credentials of a profile by running the import command again.
If the accesskey and secrets was downloaded as a csv file, it can be used in the import command.
    #aws configure import --csv file://iam_admin_accessKeys.csv
This command may fail, but can be fixed by editing the csv file
    #vim aws_userforiam_accessKeys.csv
        ->Prepend first line with "User Name"
        ->Prepend second line with "default" or with the username
    #aws configure import --csv file://iam_admin_accessKeys.csv
    #aws configure list-profiles
    #aws configure list
        ->If you have not used "default", use the flag --profile=username
    #cat ~/.aws/credentials
To interactively set accesskeys and secrets for the profile foo
    #aws configure --profile foo

Lets verify if the credentials work. Lets list create and destroy an s3 bucket
    #aws s3 help
    #aws s3 ls
    #aws s3 mb "s3://mynewbucket-willwork"
        ->If default region was not configured, use the flag --region=us-east-1
You can check aws console if the bucket was created or not
    #aws s3 ls
    #aws s3 rb "s3://mynewbucket-willwork"
    #aws s3 ls

Lets start using terraform to create and manage resources in this aws account of ours.
    #mkdir 01-aws-s3 && cd 01-aws-s3
Use the aws provider and set it up with shared config and creds. Refer https://registry.terraform.io/providers/hashicorp/aws/latest/docs#shared-configuration-and-credentials-files
    #vim providers.tf
The config in aws providers block is optional if you are using default profile and default paths.
This would also be a good spot to add default_tags that you would want all your resources to inherit.
Initialize the project by installing the provider plugin
    #terraform init
Browse the docs for resources and data sources supported by the aws provider.

Lets start off by managing an S3 bucket. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket
Create your first S3 bucket using the aws_s3_bucket resource with only its bucket argument.
    #vim s3.tf
    #terraform apply
If the bucket name is not globally unique, you will see errors like "BucketAlreadyExists" or "Header malformed : Wrong region"
Disable/Comment-out the first aws_s3_bucket resource.
Create the second aws_s3_bucket resource with bucket_prefix argument.
The bucket_prefix argument, guarantees uniqueness by appending a 26 digit number to it.
If bucket and bucket_prefix are not used, the word "terraform" is used as bucket_prefix.
The terraform aws provider generates a time based unique random number that has the format YYYYMMDDhhmmssXXXX00000001.
Include a hyphen at the end of the prefix so that the bucket_name looks nice.
Use the force_destroy argument so that we can still destroy or recreate even if bucket is not empty or is processing uploads/downloads.
    #vim s3.tf
    #terraform apply
    #terraform state list
    #terraform state show aws_s3_bucket.second
    #aws s3api list-buckets
Create output block named mys3_id_second to display the name of the s3 bucket that was created
    #terraform console
        #aws_s3_bucket.second.id
    #vim output.tf
    #terraform apply
    #terraform output
    #terraform output mys3_id_second
The tags argument in aws_s3_bucket resource allows us to add new tags and/or override the values of any default_tags.
    #vim s3.tf
    #terraform apply
    #terraform state show aws_s3_bucket.second
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_s3_bucket.second") | .values | with_entries( select( .key | startswith("tag") ) )'
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_s3_bucket.second") | .values.tags'
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_s3_bucket.second") | .values.tags_all'
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="aws_s3_bucket.second") | .values | {tags: .tags, tags_all: .tags_all}'
    #aws s3api get-bucket-tagging help
    #aws s3api get-bucket-tagging --bucket $(aws s3api list-buckets --query "Buckets[].Name" --output text)
    #aws s3api get-bucket-tagging --bucket $(terraform output -raw mys3_id_second )
    #terraform output -raw mys3_id_second | xargs -I?? aws s3api get-bucket-tagging --bucket ??
    #terraform output -raw mys3_id_second | { read OUT; aws s3api get-bucket-tagging --bucket $OUT; }
Create variable blocks mys3_prefix, bucket_force_destroy and bucket_extra_tags to handle the inputs.
Set default values for each variable. Its safer to set force_destroy as false but lets use true for our training project.
    #vim variable.tf
    #terraform console
        #var.mys3_prefix
        #var.bucket_force_destroy
        #var.bucket_extra_tags
Create a local mys3_prefix_hyphen thats just a hyphenated version of mys3_prefix variable
Update the argument values of aws_s3_bucket to use these variables and locals.
    #vim s3.tf
    #terraform console
        #local.mys3_prefix_hyphen
    #terraform apply
We will look at bucket versioning and uploading into the bucket later.

S3 buckets need globally unique name. We got this by adding a time+random number to the name using bucket prefix.
Lets use Terraforms Random provider to create random resources that can be applied wherever we need uniqueness. https://registry.terraform.io/providers/hashicorp/random/latest/docs
    #vim providers.tf
    #terraform init
Lets create a random_id  resource with a byte_length of 7. byte_length is required and defines the size of the random generated binary code.
Lets create a random_pet resource with a length      of 3. length has a default value of 2 readable words.
    #vim s3.tf
    #terraform apply
    #terraform state show random_pet.third
    #terraform state show random_id.third
None of the attributes you see of random_id.third resource is the actual random code.
All attributes are encodings and/or conversions of some non-printable code.
I would be using the b64_url attribute as it has letters and numbers.
If you are curious to see the real code of random_id,
    #terraform show -json | jq '.values.root_module.resources[] | select(.address=="random_id.third") |.values'
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std'
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std' | base64 -d | hexdump -C
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std' | base64 -d | xxd -p
    #terraform show -json | jq -r '.values.root_module.resources[]|select(.address=="random_id.third")|.values.b64_std' | base64 -d | xxd -p | { read hex_val; echo $((16#$hex_val)); }
Creating an output block for random_id.third.b64_std would have allows us avoid the long jq. But we cannot isolate it when we use the prefix argument in random_id.
Set the prefix argument in random_id and random_pet resources with mys3_prefix_hyphen and mys3_prefix.
    #vim s3.tf
Lets also use variables to manage the length of random_id and random_pet
    #vim variables.tf
    #terraform console
        #var.suffix_string_length
        #var.suffix_word_count
    #terraform apply
Lets create new aws_s3_buckets whose names would be random_id.third.b64_url and random_pet.third.id. We can cycle through them using for each
Although the b64_url looks usable, s3 buckets names cannot have uppercase and/or underscore(_).
So, substitute "_" with "-" with the replace function and also use the lower function on it. https://developer.hashicorp.com/terraform/language/functions/replace
    #terraform console
        #random_id.third.b64_url
        #lower(replace(random_id.third.b64_url, "_", "-"))
        #toset(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"])
    #vim s3.tf
    #terraform apply
    #terraform state list
    #aws s3api list-buckets
Create an output block to return the names of the buckets created.
    #terraform console
        #aws_s3_bucket.third
        #values(aws_s3_bucket.third)[*].id
    #vim output.tf
    #terraform apply
    #terraform output mys3_id_third
    #aws s3api list-buckets
We will look at random_uuid later.

If you had followed along with you would have created 3 buckets without any terraform fail/error.
But there is fault that was hidden while terraform apply after each minor change. Lets uncover this fault
    #terraform destroy
    #terraform apply
Terraform apply will fail due to error at for_each of third aws_s3_buckets resource.
Arguments of a resource are allowed to refer attributes of other resources, this would be considered as an implicit dependency.
Terraform would delay the creation of the downstream dependent resource until the upstream resource is created/modified and its attributes become available.
But terraform plan/apply fails here, when for_each the meta-argument is referring to resource attributes. Why?
(for_each + each.key) and (count + count.index) are evaluated and must be known before the creation of the dependency graph.
Their result decides what the names and labels of the resources must be, without which is dependency graph will be incomplete.
There are 3 solutions for this for_each error.
  Solution 1 : Avoid for_each
    We could create 2 seperate aws_s3_bucket blocks. One of them would get bucket name from random_id, and the Next one would use random_pet.
    I wont use this as it too simple for my taste.
  Solution 2 : Map of static keys:
    To correct the error we replace the set in for_each with a map of known/static keys.
        #terraform destroy
        #terraform console
            #toset(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"])
            #tomap({ third_id = "${lower(replace(random_id.third.b64_url, "_", "-"))}", third_pet = "${random_pet.third.id}" })
    Defining the map as a local will improve code readability. Name of the bucket should now be each.value
        #vim s3.tf
        #terraform apply
    Even though we switched from a set to a map in aws_s3_bucket.third, the value of output.mys3_id_third works with the same expression.
        #terraform state list
        #aws s3api list-buckets
        #terraform output
  Solution 3 : Count of known length
    Another solution is to replace the "for_each = toset()" with count whose value is the length of a list.
        #terraform destroy
        #terraform console
            #toset(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"])
            #length(tolist(["${lower(replace(random_id.third.b64_url, "_", "-"))}", "${random_pet.third.id}"]))
    Define the list as a local. Set the count as length(local.list) and the name of buckets as list items at count.index
        #vim s3.tf
        #terraform apply
    The value of output.mys3_id_third no longer needs the values() function.
        #terraform console
            #aws_s3_bucket.third[*].id
        #vim output.tf
        #terraform apply
        #terraform state list
        #aws s3api list-buckets
        #terraform output
Your choice on Solution 2 or 3 should depend on your preference of named vs numbered terraform resource object. I prefer number so I choose solution 3.

We will be trying out random_uuid, bucket versioning and uploading into bucket soon.
But before that, I want to address a logical issue with the project so far.
To create a bucket with a unique name and we have used 3 methods.
And this resulted in the creation of three unique buckets, where we only needed one.
We need to be able choose one of three suffix methods to create one bucket.
Conditional Expression is our solution https://developer.hashicorp.com/terraform/language/expressions/conditionals.
Create a string type variable that enables us to choose the kind of suffix.
Apply a validation condition so that an error_message is shown for requests of unknown suffix_type. https://developer.hashicorp.com/terraform/language/validate
The contains function allows us to define a list of allowed suffix_type, ie, [all, none, time, words, string]. https://developer.hashicorp.com/terraform/language/functions/contains
    #vim variables.tf
    #terraform console
        #var.suffix_type
We use condition expression to check var.suffix_type. If it matches a keyword, return 1, else return 0.
    #terraform console
        #var.suffix_type=="none" ? true : false
        #var.suffix_type=="none" ? "on" : "off"
        #var.suffix_type=="none" ? toset(["on"]) : toset([])
        #var.suffix_type=="none" ? 1 : 0
        #var.suffix_type=="string"||var.suffix_type=="all" ? 1 : 0
        #var.suffix_type=="words"||var.suffix_type=="all" ? 1 : 0
        #(var.suffix_type=="time"||var.suffix_type=="all") ? 1 : 0
I prefer count to evaluate conditional expressions. So, I will use 1 as true_val and 0 as false_val.
for_each can also evaluate conditional expressions. If used, toset([]), ie, an empty set must be used as false_val
Uncomment aws_s3_bucket.first and update its arguments to use var.mys3_prefix, var.bucket_force_destroy and var.bucket_extra_tags.
    #vim s3.tf
Create an output block to return the bucket name of aws_s3_bucket.first as well.
    #vim output.tf
We define count in aws_s3_bucket.first, aws_s3_bucket.second, random_id.third, random_pet.third to use their respective conditional expression.
    #vim s3.tf
Conditional expression in aws_s3_bucket.third will be applied after some considerations.
    #terraform plan
    #terraform plan -var 'suffix_type=none'
Because of this change, the 4 individual resource objects have become 4 lists of objects that contain a singluar element or might be empty.
Also, evaluating expressions on resources that may or may not exist will causes errors during terraform plan/apply. We a error handling function
Try function handles mutiple expressions and will have the result of the first expression that doesnt fail. https://developer.hashicorp.com/terraform/language/functions/try
Avoid using the splat operator (ie [*] or wildcard) when you know the list will ever have only one element.
Pass the current expressions of output.mys3_id_first, output.mys3_id_second and local.id_list_third as the primary to the try function.
Update the primary expression of all try functions to evaluate the 0th element of their upstream resources.
And set null as the secondary/fallback expression in each try function.
    #vim s3.tf
    #vim output.tf
    #terraform plan
    #terraform plan -var 'suffix_type=none'
Even when suffix_type=none, you will see aws_s3_bucket.third is planning to create 2 buckets without names.
This is because even though random_id.third, random_pet.third are switched off, local.id_list_third has a length of 2 with null elements.
We need count to be 2 when suffix_type is all, 1 when suffix_type is words or string, 0 when suffix_type is none or unknown
    #terraform console
        #(var.suffix_type=="all") ? 2 : ((var.suffix_type=="words"||var.suffix_type=="string") ? 1 : 0)
    #vim s3.tf
Also use compact function to remove null from local.id_list_third. https://developer.hashicorp.com/terraform/language/functions/compact
Now, aws_s3_bucket.third can use the random_pet.third.id as suffix when count is 1 instead of bucket name being empty.
    #vim s3.tf
    #terraform apply
    #aws s3api list-buckets
    #terraform apply -var 'suffix_type=words'
    #terraform apply -var 'suffix_type=string'
    #terraform apply -var 'suffix_type=time'
    #aws s3api list-buckets
When using words or string suffix_type, you would see output.mys3_id_second and output.mys3_id_first do not exist. Its because try function uses null.
Using null as a fallback for output.mys3_id_third wont work as try function does not recognise an empty list of objects as a failure/error.
A conditional expression can apply null as values if the length of aws_s3_bucket.third is 0
    #terraform console
        #length(aws_s3_bucket.third)==0 ? null : aws_s3_bucket.third[*].id
        #length(aws_s3_bucket.third)>0 ? aws_s3_bucket.third[*].id : null
    #vim output.tf
    #terraform apply -var 'suffix_type=time'
    #terraform output mys3_id_third
    #aws s3api list-buckets

Lets look at generating uuids which we will later use as bucket suffix
Lets create a bucket with random_uuid resource from the random provider. prefix argument in not supported.
    #vim s3.tf
Lets include "uuid" as suffix_type and use it enable or disable random_uuid.fourth
    #vim variables.tf
    #vim s3.tf
    #terraform apply -var 'suffix_type=uuid'
    #terraform state show random_uuid.fourth[0]
An inconvenience of the time, string, word, and uuid suffix is that the bucket name uniqueness came at the cost of predictability.
If the bucket name had to be used by another team, they will have to wait for you perform an apply and share the output that has the bucket id.
uuid version 5 operates on namespace and name inputs and will always generate the same uuid. https://developer.hashicorp.com/terraform/language/functions/uuidv5
Lets generate uuid using the oid namespace for our var.mys3_prefix "tf-bucket" via linux bash shell and via terraform console.
    #uuidgen -s -n @oid -N "tf-bucket"
    #terraform console
        #uuidv5("oid",var.mys3_prefix)
We get the same 32-digit hex code as our uuid.
Lets include "uuidv5_int" as suffix_type and use it as a condition to set local.uuidv5_int to use uuidv5 function or be null.
    #vim variables.tf
    #vim s3.tf
    #terraform console -var 'suffix_type=uuidv5_int'
        #local.uuidv5_int
If uuidv5 or some other processing you need is not availble within terraform, we have the option to run it and import its result.
Using hashicorp/external provider allows us to specify the program and use its result as a data source. https://registry.terraform.io/providers/hashicorp/external/latest/docs
    #vim providers.tf
    #terraform init
The result of the external data source must be json formated for terraform to be able to consume.
    #uuidgen -s -n @oid -N "tf-bucket" | jq -R '{uuid_full: .}'
Create a data source block to run uuidgen on mys3_prefix using bash. https://registry.terraform.io/providers/hashicorp/external/latest/docs/data-sources/external
    #vim s3.tf
Allow "uuidv5_ext" to be a valid suffix_type and use it to enable data.external.fourth
    #vim variables.tf
    #vim s3.tf
    #terraform apply -var 'suffix_type=uuidv5_ext'
    #terraform state show data.external.fourth[0]
Now lets create buckets using these uuids
Create a local.id_list_fourth that tries to get random_uuid.fourth[0].id, data.external.fourth[0].id and local.uuidv5_int, uses them as list elements and compacts it.
    #terraform console
        #tolist([ "${try(random_uuid.fourth[0].id, null)}", "${try(local.uuidv5_int, null)}", "${try(data.external.fourth[0].result.uuidv5_ext, null)}" ])
Actually data.external.fourth[0].id and local.uuidv5_int are the same and local.uuidv5_int already fallback to null. So combine these 2.
    #terraform console
        #tolist(["${try(random_uuid.fourth[0].id, null)}", "${try(data.external.fourth[0].result.uuidv5_ext, local.uuidv5_int)}" ])
    #vim s3.tf
Create a aws_s3_bucket resource that creates 2 buckets if suffix_type is all or 1 bucket is suffix_type starts with uuid.
Use local.id_list_fourth elements with mys3_prefix_hyphen as the bucket argument. Also include the tags and force_destroy arguments.
    #vim s3.tf
    #terraform apply -var 'suffix_type=uuid'
    #aws s3api list-buckets
Create output block to return the list of created bucket names if aws_s3_bucket has created any
    #terraform console
        #length(aws_s3_bucket.fourth)>0 ? aws_s3_bucket.fourth[*].id : null
    #terraform apply -var 'suffix_type=uuidv5_ext'
    #terraform output mys3_id_fourth
    #terraform apply
    #aws s3api list-buckets --output table
    #terraform destroy

Improvement task:
What if the 32 digit uuid is too big or not pretty enough to be used as bucket name?
You can split it into 5 elements of 8-4-4-4-12 digits and use the first_block or last_block as suffix
You can also remove the hyphens and count any number of digits from the front or back and us that as suffix for your bucket.
You may use this bash command as your uuidv5_ext and maybe figure out the same results for uuidv5_int and random uuid.
    #uuidgen -s -n @oid -N "tf-bucket" | jq -R \
      '{
          uuidv5_ext_full: .,
          uuidv5_ext_fblock: (split("-")[0]),
          uuidv5_ext_lblock: (split("-")[4]),
          uuidv5_ext_fcount: (gsub("-"; "")[:6]),
          uuidv5_ext_bcount: (gsub("-"; "")[-6:]),
      }'
Figure out the terraform functions you would use to do the same conversions for local.uuidv5_int and random_uuid.fourth

Now lets enable versioning on any/all buckets that we create.
First lets have a variable bucket_versioning created with default value of true.
    #vim variables.tf
    #terraform console
        #var.bucket_versioning
Although versioning is an argument in aws_s3_bucket resource, terraform has depreciated it as it cannot detect state drift for this attribute/argument.
Using aws_s3_bucket_versioning resource is the recomended methods to enable/disable versioning.
Create a local to hold a list of any/all bucket names to be cycled in the bucket argument in aws_s3_bucket_versioning.
As we need to combine 2 strings with 2 lists, we will use setunion function with compact. https://developer.hashicorp.com/terraform/language/functions/setunion
This combined list will be a good addition to our output as well.
    #terraform console
        #setunion([try(aws_s3_bucket.first[0].id, null)], [try(aws_s3_bucket.second[0].id, null)], aws_s3_bucket.third[*].id, aws_s3_bucket.fourth[*].id)
        #compact(setunion([try(aws_s3_bucket.first[0].id, null)], [try(aws_s3_bucket.second[0].id, null)], aws_s3_bucket.third[*].id, aws_s3_bucket.fourth[*].id))
    #vim s3.tf
    #vim output.tf
Create another local to be 5 when suffix_type is all and var.bucket_versioning is true, or 1 if only var.bucket_versioning is true else be 0
We are using 5 as it would be the number of buckets created if suffix_type is all (time+word+string+uuid+uuidv5). Create a local that describes why 5 is used.
    #terraform console
        #(var.bucket_versioning && var.suffix_type=="all") ? 5 : ( var.bucket_versioning ? 1 : 0)
    #vim s3.tf
Create the aws_s3_bucket_versioning resource. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_versioning.
Set count to local.bucket_versioning_count
Set bucket to cycle through local.mys3_id_all.
Create versioning_configuration argument block and set status as Enabled.
    #vim s3.tf
    #terraform apply
    #aws s3api get-bucket-versioning help
    #aws s3api list-buckets | jq -r '.Buckets[].Name'
    #aws s3api list-buckets | jq -r '.Buckets[].Name' | xargs -L 1 aws s3api get-bucket-versioning --output text --bucket
    #terraform apply -var 'bucket_versioning=false'
    #aws s3api list-buckets | jq -r '.Buckets[].Name' | xargs -L 1 aws s3api get-bucket-versioning --output text --bucket
    #terraform apply -var 'suffix_type=uuidv5_int'
    #aws s3api list-buckets | jq -r '.Buckets[].Name'
    #aws s3api list-buckets | jq -r '.Buckets[].Name' | xargs -L 1 aws s3api get-bucket-versioning --output text --bucket
    #terraform destroy

We will be using this project as a child_module in other terraform_aws projects to prepare backends.
But before that, lets look basic functionality of s3 buckets,ie to store files.
Create a variable bucket_upload which should accept a list of files. Default value should be an emptyset.
    #vim variables.tf
    #terraform console
        #var.bucket_upload
We will be using that aws_s3_object resource to upload a source, into bucket as key.
We can use either count or for_each (never both) to cycle through buckets or sources. When suffix_type is all, we need to be able to cycle through 2 lists.
The setproduct function combines 2 lists to create a new list containing sublists of 2 elements each.
Create one local that is a setproduct of local.mys3_id_all and var.bucket_upload and another local that is the product of lengths of each list.
    #terraform console
        #setproduct(["A","B","C","D",],[1,2,3])
        #setproduct(local.mys3_id_all, var.bucket_upload)
    #vim s3.tf
Create local.bucket_upload_count to be product of lengths when suffix_type is all and var.bucket_upload is not empty, or the length of var.bucket_upload
    #terraform console
        #(length(var.bucket_upload)>0 && var.suffix_type=="all") ? local.bucket_upload_max : length(var.bucket_upload)
    #vim s3.tf
Create aws_s3_object resource. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_object
Set count as local.bucket_upload_count.
Set bucket argument to cycle through local.bucket_upload_set and pick the 0th element of each subset.
Set source argument to cycle through local.bucket_upload_set and pick the 1st element of each subset.
Set key argument to prefix a directory_name to the basenames of the 1st element of every subset of local.bucket_upload_set
    #vim s3.tf
    #terraform apply -var 'suffix_type=string' -var 'bucket_upload=["../aws.txt"]'
    #terraform state list
    #aws s3api list-buckets | jq -r '.Buckets[].Name'
    #aws s3api list-objects-v2 help
    #aws s3api list-buckets | jq -r '.Buckets[].Name' | xargs -L 1 aws s3api list-objects-v2 --bucket | jq -r '.Contents[] | [.LastModified, .Size, .Key ] | @tsv'
    #terraform apply -var 'suffix_type=string' -var "bucket_upload=[$(printf '"%s", ' ../*.txt)]"
        ->Double Quotes are needed for var.bucket_upload
    #printf '"%s", ' ../*.txt | xargs -d '\n' -I?? terraform apply -var 'suffix_type=string' -var 'bucket_upload=[??]' -auto-approve
    #printf '"%s", ' ../*.txt | { read list; terraform apply -var 'suffix_type=string' -var "bucket_upload=[$list]" -auto-approve;}
        ->Double Quotes are needed for var.bucket_upload
    #aws s3api list-buckets | jq -r '.Buckets[].Name' | xargs -L 1 aws s3api list-objects-v2 --bucket | jq -r '.Contents[] | [.LastModified, .Size, .Key ] | @tsv'
    #terraform apply -var 'bucket_upload=[$(ls -Qm ../*.txt)]'
    #ls -Qm ../*.txt | xargs -d '\n' -I?? terraform apply -var 'bucket_upload=[??] -auto-approve'
    #ls -Qm ../*.txt | { read list; terraform apply -var "bucket_upload=[$list]" -auto-approve; }
        ->Double Quotes are needed for var.bucket_upload
    #aws s3api list-buckets | jq -r '.Buckets[].Name'
    #aws s3api list-buckets | jq -r '.Buckets[].Name' | xargs -L 1 aws s3api list-objects-v2 --bucket | jq -r '.Contents[] | [.LastModified, .Size, .Key ] | @tsv'
    #terraform destroy

We will be using the learnings from this project to create child_module in other aws projects to prepare backends.
Thats all about 01-aws-s3 project . Before we commit, lets format our code
    #terraform fmt -check
    #terraform fmt -diff
